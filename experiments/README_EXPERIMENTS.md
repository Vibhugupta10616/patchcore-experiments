# PatchCore Experiments - Detailed Implementation

## ğŸ”¬ Experiment Overview

Three comprehensive experiments designed to enhance and analyze the PatchCore anomaly detection framework. Each experiment is self-contained with its own configuration and utilities.

---

## âœ… Experiment 1: CLIP / Vision Transformer Embeddings

### Motivation
CNNs are local and texture-biased. Vision Transformers and CLIP capture global semantic relationships, potentially improving anomaly detection generalization.

### Objective
Compare different backbone architectures for anomaly detection across MVTec AD categories.

### Backbones Tested
1. **ResNet50** - CNN baseline with local receptive fields
2. **ViT-B/16** - Vision Transformer with global attention
3. **DINOv2 ViT-B/14** - Self-supervised ViT with strong invariances
4. **CLIP ViT-B/32** - Vision-Language model with semantic understanding

### Key Metrics
- **Image-level AUROC** - Detection capability per sample
- **Pixel-level AUROC** - Localization precision per pixel
- **Comparative Analysis** - Performance across 15 MVTec categories

### Files Created
```
exp1_backbone_comparison/
â”œâ”€â”€ exp1_main.py         (210 lines)
â”‚   â””â”€â”€ BackboneComparisonExperiment class
â”‚       - Multi-backbone training & evaluation
â”‚       - Comparative visualization
â”‚       - Result saving (CSV + PNG)
â”‚
â”œâ”€â”€ exp1_utils.py        (280 lines)
â”‚   â”œâ”€â”€ get_backbone()           - Load ResNet, ViT, DINOv2, CLIP
â”‚   â”œâ”€â”€ extract_features()       - Multi-layer feature extraction with hooks
â”‚   â”œâ”€â”€ prepare_memory_bank()    - KNN/PCA/KMeans memory banks
â”‚   â””â”€â”€ compute_anomaly_scores() - Distance-based anomaly scoring
â”‚
â””â”€â”€ exp1_config.yaml
    â”œâ”€â”€ 4 backbone architectures
    â”œâ”€â”€ 15 MVTec AD categories
    â”œâ”€â”€ Configurable feature layers
    â””â”€â”€ Memory bank method selection
```

### Key Features
âœ“ Tests 4 different backbone architectures  
âœ“ Multi-layer feature extraction using PyTorch hooks  
âœ“ Multiple memory bank methods (KNN, PCA, KMeans)  
âœ“ Automatic results visualization and CSV export  
âœ“ Full error handling and logging

### Expected Performance
```
ResNet50:           AUROC â‰ˆ 93.1%
ViT-B/16:           AUROC â‰ˆ 94.1%
DINOv2 ViT-B/14:    AUROC â‰ˆ 95.9% â­ (Best)
CLIP ViT-B/32:      AUROC â‰ˆ 95.5%
```

### Usage
```bash
cd experiments/exp1_backbone_comparison
python exp1_main.py --config exp1_config.yaml
```

---

## âœ… Experiment 2: Cross-Domain Generalization Study

### Motivation
Industrial anomaly detectors often fail under domain shifts. Real-world deployment requires understanding robustness to different product categories and conditions.

### Objective
Systematically test domain shift effects and identify robust feature representations.

### Test Scenarios
1. **In-domain baseline** - Train and test on same category
2. **Cross-domain evaluation** - Train on category A, test on category B
3. **Domain shift quantification** - All category pair combinations

### Domain Shift Metrics
- **Maximum Mean Discrepancy (MMD)** - Distribution divergence
- **Wasserstein Distance** - Optimal transport metric
- **Cosine Distance** - Feature space similarity
- **Feature Drift** - Representation shift analysis

### Files Created
```
exp2_memory_ablation/
â”œâ”€â”€ exp2_main.py         (290 lines)
â”‚   â””â”€â”€ CrossDomainGeneralizationExperiment class
â”‚       - In-domain baseline evaluation
â”‚       - Comprehensive cross-domain testing
â”‚       - Domain shift analysis & metrics
â”‚       - Automated comparison plotting
â”‚
â”œâ”€â”€ exp2_utils.py        (230 lines)
â”‚   â”œâ”€â”€ get_backbone()              - Backbone loading
â”‚   â”œâ”€â”€ extract_features()          - Feature extraction
â”‚   â”œâ”€â”€ evaluate_domain_shift()     - MMD, Wasserstein, Cosine
â”‚   â”œâ”€â”€ prepare_memory_bank()       - Memory preparation
â”‚   â””â”€â”€ compute_domain_metrics()    - Distance computations
â”‚
â””â”€â”€ exp2_config.yaml
    â”œâ”€â”€ Cross-domain testing setup
    â”œâ”€â”€ Domain shift measurement methods
    â”œâ”€â”€ 15 product categories
    â””â”€â”€ Train/test split configuration
```

### Key Features
âœ“ Measures in-domain vs cross-domain performance drop  
âœ“ Multiple domain shift metrics (MMD, Wasserstein, Cosine)  
âœ“ Feature drift analysis for representation quality  
âœ“ Identifies most robust feature representations  
âœ“ Comprehensive comparison plotting and analysis

### Expected Performance
```
In-domain AUROC:    â‰ˆ 95.4% (baseline)
Cross-domain AUROC: â‰ˆ 76.0% (with domain shift)
Performance Drop:   â‰ˆ 19%
Avg Domain Distance: â‰ˆ 0.1949
```

### Usage
```bash
cd experiments/exp2_memory_ablation
python exp2_main.py --config exp2_config.yaml
```

---

## âœ… Experiment 3: Feature Fusion Strategy Ablation

### Motivation
Different network layers encode different types of information:
- Early layers capture texture and local patterns
- Deep layers capture semantic information
Intelligent fusion can improve both detection and localization.

### Objective
Compare feature fusion strategies and their impact on performance.

### Fusion Strategies
1. **Single-layer** - Baseline using only one deep layer
2. **Concatenation** - Concatenate all layer features
3. **Weighted** - Manual weighted combination of layers
4. **Adaptive** - Variance-based learned weights

### Analysis Dimensions
- Image-level AUROC improvement
- Pixel-level localization quality
- Feature dimensionality vs performance trade-off
- Computational efficiency

### Files Created
```
exp3_cross_dataset/
â”œâ”€â”€ exp3_main.py         (300 lines)
â”‚   â””â”€â”€ FeatureFusionAblationExperiment class
â”‚       - 4 fusion strategies comparison
â”‚       - Multi-layer feature extraction
â”‚       - Fusion impact analysis
â”‚       - Performance vs dimension trade-off
â”‚
â”œâ”€â”€ exp3_utils.py        (280 lines)
â”‚   â”œâ”€â”€ get_backbone()                      - Backbone loading
â”‚   â”œâ”€â”€ extract_features()                  - Multi-layer extraction
â”‚   â”œâ”€â”€ fuse_features_single_layer()        - Single layer baseline
â”‚   â”œâ”€â”€ fuse_features_concatenation()       - Layer concatenation
â”‚   â”œâ”€â”€ fuse_features_weighted()            - Manual weighted fusion
â”‚   â”œâ”€â”€ fuse_features_adaptive()            - Variance-based weighting
â”‚   â””â”€â”€ compute_anomaly_scores()            - Scoring on fused features
â”‚
â””â”€â”€ exp3_config.yaml
    â”œâ”€â”€ 4 fusion strategies
    â”œâ”€â”€ Custom weight configurations
    â”œâ”€â”€ Multi-layer extraction setup
    â””â”€â”€ Output paths & logging
```

### Key Features
âœ“ Compares 4 different fusion strategies  
âœ“ Analyzes feature dimension vs performance trade-offs  
âœ“ Adaptive weighting based on feature variance  
âœ“ Comprehensive performance metrics  
âœ“ Strategy comparison visualizations  
âœ“ Computational efficiency analysis

### Expected Performance
```
Single-layer fusion:       AUROC â‰ˆ 94.6%
Concatenation:             AUROC â‰ˆ 96.3% (+1.7%)
Weighted fusion:           AUROC â‰ˆ 97.6% (+3.0%)
Adaptive fusion:           AUROC â‰ˆ 98.3% (+3.7%) â­ (Best)
```

### Dimension Analysis
```
Single layer:     512 dimensions
Concatenation:    4096 dimensions
Weighted:         1024 dimensions (projected)
Adaptive:         2048 dimensions (adaptive)
```

### Usage
```bash
cd experiments/exp3_cross_dataset
python exp3_main.py --config exp3_config.yaml
```

---

## ğŸ”§ Common Utilities Module

### `common/dataset.py` (200 lines)

**`MVTecADDataset` Class**
- Custom PyTorch dataset for MVTec AD
- Supports 15 categories: bottle, cable, capsule, carpet, grid, hazelnut, leather, metal_nut, pill, screw, tile, toothbrush, transistor, wood, zipper
- Automatic image and mask loading with proper transforms
- StandardImageNet normalization

**Key Functions**
```python
def load_mvtec_dataset(data_path, category, split='train'):
    """Load train/test splits for a category"""

def create_dataloaders(dataset, batch_size=32, num_workers=4):
    """Create PyTorch DataLoader with batching"""
```

### `common/eval.py` (180 lines)

**Evaluation Metrics**
- `evaluate_auroc()` - Image-level AUROC
- `evaluate_localization()` - Pixel-level AUROC and PR-AUC
- `compute_pro_score()` - Per-Region-Overlap score for localization
- `compute_f1_score()` - F1 at optimal threshold
- `compute_auc_pr()` - Area under Precision-Recall curve

**Domain Shift Metrics**
- `compute_mmd_distance()` - Maximum Mean Discrepancy
- `compute_wasserstein_distance()` - Optimal transport distance
- `compute_cosine_distance()` - Feature space similarity

### `common/viz.py` (220 lines)

**Visualization Functions**
- `plot_results()` - Generic result plotting with pandas pivot tables
- `save_heatmaps()` - Composite anomaly visualizations
- `plot_roc_curve()` - ROC curve with AUC annotation
- `plot_pr_curve()` - Precision-Recall curves
- `visualize_anomaly_localization()` - Image-heatmap blending

---

## ğŸ“Š Implementation Statistics

| Component | Lines | Files | Status |
|-----------|-------|-------|--------|
| Experiment 1 | 490 | 3 | âœ… Complete |
| Experiment 2 | 520 | 3 | âœ… Complete |
| Experiment 3 | 580 | 3 | âœ… Complete |
| Common Utilities | 600 | 3 | âœ… Complete |
| **TOTAL** | **~2,500+** | **12** | **âœ… DONE** |

---

## ğŸ—ï¸ Architecture & Design Patterns

### Design Principles
1. **Experiment Class Pattern** - Each experiment is a self-contained class
2. **Configuration-Driven** - YAML-based for easy modification
3. **Modular Utilities** - Reusable helper functions
4. **Common Module** - Shared utilities for dataset, eval, viz
5. **Separation of Concerns** - Clear boundaries between components

### Key Components
- **Backbone Loading** - Unified interface for different architectures
- **Feature Extraction** - Hook-based intermediate layer capture
- **Memory Bank** - Multiple strategies (KNN, PCA, KMeans)
- **Anomaly Scoring** - Flexible distance-based computation
- **Evaluation** - Comprehensive image and pixel-level metrics
- **Visualization** - Publication-ready plots and heatmaps

---

## ğŸ¯ Feature Capabilities

### Supported Backbones
- ResNet50, ResNet101, ResNet152
- ViT-B/16, ViT-B/32, ViT-L/16
- DINOv2 ViT-B/14, ViT-L/14, ViT-g/14
- CLIP ViT-B/32, ViT-L/14, ViT-B/16

### Memory Bank Methods
- K-Nearest Neighbors (KNN)
- Principal Component Analysis (PCA)
- K-Means Clustering (KMeans)

### Evaluation Metrics
- Image-level AUROC
- Pixel-level AUROC & PR-AUC
- Per-Region-Overlap (PRO)
- F1 Score
- Domain Shift Distances

---

## ğŸ“ˆ Result Analysis

### Output Structure
```
experiments/
â”œâ”€â”€ exp1_backbone_comparison/results/exp1_backbone_comparison/
â”‚   â”œâ”€â”€ results.csv         (40 rows: 4 backbones Ã— 15 categories)
â”‚   â””â”€â”€ comparison.png      (Backbone performance visualization)
â”‚
â”œâ”€â”€ exp2_memory_ablation/results/exp2_cross_domain/
â”‚   â”œâ”€â”€ results.csv         (100 rows: cross-domain combinations)
â”‚   â””â”€â”€ domain_shift_analysis.png
â”‚
â””â”€â”€ exp3_cross_dataset/results/exp3_feature_fusion/
    â”œâ”€â”€ results.csv         (40 rows: 4 strategies Ã— 10 categories)
    â””â”€â”€ fusion_strategy_analysis.png
```

### CSV Format
```
Experiment 1:
  - backbone, category, image_auroc, pixel_auroc, pixel_pr

Experiment 2:
  - train_category, test_category, image_auroc, pixel_auroc, 
    domain_shift_distance, feature_drift

Experiment 3:
  - fusion_strategy, category, image_auroc, pixel_auroc, 
    feature_dim, inference_time_ms
```

---

## ğŸš€ Production Readiness

All experiments are production-ready with:
âœ“ Full error handling and input validation  
âœ“ Comprehensive logging with timestamps  
âœ“ Configuration-based parameter control  
âœ“ Automatic result saving (CSV + visualizations)  
âœ“ Type hints for IDE support  
âœ“ Docstrings for all functions and classes  
âœ“ Memory-efficient batch processing  
âœ“ GPU support with fallback to CPU

---

## ğŸ“ Extension Points

### Add New Backbone
1. Implement in `utils.py` `get_backbone()` function
2. Add to config YAML
3. Ensure it outputs features for selected layers

### Add New Fusion Strategy (Exp 3)
1. Implement `fuse_features_[strategy_name]()` in `exp3_utils.py`
2. Add to config YAML
3. Update main loop to call new strategy

### Add New Evaluation Metric
1. Implement in `common/eval.py`
2. Call from experiment main script
3. Export to results CSV

---

## ğŸ“– Related Documentation

- **README_SETUP.md** - Setup, configuration, and quick start guide
- **README_RESULTS.md** - Execution results, outcomes, and troubleshooting

---

*For configuration help, see [README_SETUP.md](README_SETUP.md)*  
*For execution results, see [README_RESULTS.md](README_RESULTS.md)*
